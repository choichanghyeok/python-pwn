1. 13강

# 오차함수

학습 개념

학습데이터는 입력(x)인 공부시간에 비례해서 출력(y)인 시험성적도 증가하는 경향이있음

즉 입력(x)와 출력(y)은 y=Wx+b 형태로 나타낼수있다.

트레이닝 데이터의 특성을 가장 잘 표현 할 수 있는 가중치 W(기울기) 바이어스b(y절편) 을 찾는 과정을 러닝(학습)이라고 부름

@@ 오차 , 가중치 , 바이어스

오차(error) = t - y = t - (Wx+b) 으로 계산

오차가 크다면 우리가 임의로 설정한 직선의 가중치와 바이어스 값이 잘못된 것이고, 오차가 작다면 
직선의 가중치와 바이어스 값이 잘된것이기 때문에 미래 값 예측도 정확 할 수있다고 예상가능하다.

머신러닝 회귀 시스템은 >> 모든 데이터의 오차 의 합이 최소가 되서, 미래값을 잘예측할수있는
가중치 W와 바이어스 b 값을 찾아야한다.



# 손실함수

트레이닝 데이터의 정답(t)과 입력(x)에 대한 계산 값 y의 차이를 모두 더해 수식으로 나타내는것

각각의 오차인 (t-y)를 모두 더해서 손실함수를 구하면 각각의 오차가 + , - 등이 동시에 존재하기 때문에 
오차의 합이 0이 나올수 있다. 즉 0이라는것이 최소 오차 값인지 아닌지를 판별하는 것이 어렵다.

손실함수에서 오차를 계산할때는 (t-y)**2 = (t-[Wx+b])**2 을 사용한다.

>> 즉 오차는 언제나 양수 이며 제곱을 하기 때문에 정답과 계산값 차이가 크다면 제곱에 의한 오차는 더
큰값을 가지게되어 머신러닝 학습에 있어 장점을 가짐.


손실함수 E(W,b)는 결국 W,b에 영향을 받는 함수다.

E(W,b)값이 작다면 정답(t,target) 과 y=Wx+b 에 의해 계산된 값의 평균 오차가 작다는 의미

평균 오차가 작다는 것은 미지의 데이터 x가 주어질 경우 확률적으로 미래의 결과값도 오차가 작을것이라 추측

이처럼 트레이닝 데이터를 바탕으로 손실함수E(W,b)가 최소값을 갖도록 (W,b)를 구하는 것이 회귀 모듈의 최종
목적이다.



14.

# 경사 하강법

@@@@@@@@@

이전시간

>>손실함수가 최소값을 가지면 미지의 데이터에 대해서 결과를 더 잘 예측할수 있다는것을 의미

@@@@@@@@@@@@

손실함수는 포물선형태로 나타남(2차함수)    >> 그래서 우리가 이 함수에서 가중치 W를 찾는게 회귀의 
최종 목적이다.

포물선에서 최소값을 찾기위해서는

1. 임의의 가중치 W 선택
2. 그 W에서의 직선의 기울기를 나타내는 미분값을 구하고
3. 그 미분값이 작아지는 방향으로 W감소시켜 나가면
4. 최종적으로 기울기가 더이상 작아지지않는 곳을 찾을수 있는데 그곳이 손신함수 E(W) 최소값임을 알수있음

이처럼 W에서의 직선의 기울기인 미분값을 이용하여, 그 값이 작아지는 방향으로 진행 하여 손실함수 최소값을 
찾는 방법을 경사하강법 이라한다.




가중치 W에서의 직선의기울기 미분값을 통해서 그값이 작아지는 곳을 통해서 최소값을 찾는것

@@@@@@@@@
이제 가중치 W를 찾는 방법



포물선 그래프에서 직선의 기울기인 라운드 w분의 라운드(편미분) 값이 양수라면 가중치 w값을 왼쪽으로 이동
w를 감소시켜야만 손실함수의 최소값에 도달함

이번에는 라운드 w분의 라운드(편미분) 값이 음수인 경우는 가중치 W값을 오른쪽으로 이동시켜야한다.
즉 손실함수를 증가시켜야만 최소값에 도달한다.

결론

양수
W 를 편미분값만큼 빼주고
음수
W를 편미분값만큼 증가시켜야함


즉 W값은 편미분값이 양수라면 현재의 W값보다 지속적으로 감소되는 방향으로 업데이트 되고
편미분값이 음수라면 뒷부분은 양수가되어 다음에 업데이트되는 W의 값은 증가되는것을 볼수있다.

@@@@@@
최적의 [W,b] 계산 프로세스

1. input 
2. 손실함수 계산
3. 손실함수 값
4.업데이트w,b  다시 1번으로


결론 정리

>> 임의의 직선 y=Wx+b를 가정후 주어진 트레이닝 데이터를 이용하여 손실함수를 계산하고 그 값이 최솟값이면
학습을 종료, 만약 최솟값이 아니라면 주어진 W와 b를 미분값을 이용하여 업데이트한후에 처음으로 돌아가
손실함수가 최소가 될떄까지 반복한다.




15. 회귀 예제 풀이


1.

	데이터 및 수식						파이썬 구현

	입력(x)	정답(t)					슬라이싱 또는 리스트등을 이용하여 입력 x와
							정답 t를 numpy 데이터 형식으로 분리



	y = Wx + b					W = numpy.random.rand(...)
							b = numpy.random.rand(...)	


	손실함수						def loss_func(..):
								y = numpy.dot(X,W) + b
								return (numpy.sum((t-y)**2)) / (len(x))

	학습률						learning_rate = 1e-3 or 1e-4 or 1e-5 .......


	가중치W,바이어스b					f = lambda x : loss_func(...)
							for step in range(6000):
		W = W - E(W,b)미분				W -= learning_rate * numerical_derivative(f,W)
		b = b - E(W,b)미분				b -= learning_rate * numerical_derivative(f,b)



오차를 계산하기 위해서는 트레이닝 데이터의 모든 입력 x에 대해 각각의 y= Wx+b 계산 해야함 이떄 입력 x 정답 t
가중치 w 모두를 행렬로 나타낸 후에 , 행렬곱을 이용하면 계싼값 y또한 행렬로 표시되어 모든 입력데이터에 대해
한번에 쉽게 계산되는 것을 알수있다.

기억해야하는식
	
	X  (점)    W + b = 	Y

	행령		행렬


       (5x1)*(1X1)	=	(5X1)




[1] 학습데이터 준비

	입력(x)		정답(t)			>> 파이썬화
	1		2			import numpy as np
	2		3			x_data = np.array([1,2,3,4,5]).reshape(5,1)
	3		4			t_data = np.array([1,2,3,4,5]).reshape(5,1)
	4		5
	5		6


[2] 임의의 직선 y = Wx+b 정의 (임의의값으로 가중치W, 바이어스b초기화)

y=Wx+b			W.np.random.rand(1,1)
			b.np.random.rand(1)

			print("W =", W, W.shape=",W.shape,"b=",b,"b.shape=",b.shape)



[3] 손실함수 E(W,b) 정의

def loss_func(x,t):
	y = np.dot(x,W) + b
	return (np.sum((t-y)**2)) / (len(x))



[4] 수치미분 numerical_derivative 및 utility 함수 정의

def numerical_derivative(f,x):
	delta_x = 1e-4
	grad = np.zeros_like(x)

	it = np.nditer(x, flags=['multi_index'], op_flags = ['readwrite']
	while not it.finished:
		idx = it.multi_index
		tem_val = x[idx]
		x[idx] = float(tem_val) + delta_x
		fx1= f(x)

		x[idx] = tem_val - delta_x
		fx2 = f(x)
		grad[idx] = (fx1 - fx2) / (2*delta_x)

		x[idx] = tem_val
		it.iternext()

	return grad





import numpy as np

# 손실함수 값 계산 함수
# 입력변수 x,y : numpy type
def error_val(x,t):
	y = np.dot(x,W) + b
	return (np.sum(t-y)**2)) / (len(x))


# 학습을 마친 후 , 임의의 데이터에 대해 미래값 예측함수
# 입력변수 x : numpy type
	def predict(x):
		y = np.dot(x,W) + b
		return y




@@학습율
			

f = lambda x : loss_func(x_aata,t,_data)
for step in range(8001):				
	W -= learning_rate * numerical_derivative(f,W)
	b -= learning_rate * numerical_derivative(f,b)

	if (step %400 == 0)
		print('step = ',step, "error value =", error_val(x_data,t_data), "W =", W, "b =", b)







